{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romainbourgeois/miniconda3/lib/python3.8/site-packages/jax/_src/lib/__init__.py:34: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/romainbourgeois/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook differ from the other one in that it merges product-services with market (somewhat similar and sometime ambiguous entities)\n",
    "\n",
    "\n",
    "The goal of this notebook id to pre-process the data to a target json format. The sequences must be split between training and testing sets. Sequences must be presented as lists of strings (tokens), each sequence must be associated with its own label.\n",
    "We are modelling 6 target entities: firm, product/service, amount of money, person, market and location. This model allows for multi-words entities, labels will differentiate between the first word and the remaining words of a multi-word entity. Tokens that do not correspond to an entity will be assigned to a label. In total, 13 labels are targets for this NER model. \n",
    "\n",
    "The text preprocessing tasks are the following. First the texts are split around the \"\\n\" tag. Lists are further seperated in sentences using nltk's sentence tokenizer method. These operations were computed so that our algorithm could process our data as sequences of sentences. Also, only the sentences incorporating a verb were kept. \n",
    "In addition, '?' '@' and '®' are removed from the dataset. This was important because some entities were followed by these characters. Although one could argue that question marks are very informative, there are extremely rare in financial documents. If we were to keep them, the algorithm would infer the entity type to be linked to the presence of the question mark and is hence not worth the trade off. Some entities were also fully put into uppercase and are hence put back to title format (keeping the first letter only). There is however a little trade-off to the extent to which we can remove upper-case letters except the first one: acronyms. \n",
    "English language is also constituted of \"contractions\" such as 'won't' instead of 'will not'. The major ones were removed in the off chance that it would make training faster. Unicode errors, urls, emails and phone numbers were also removed and replaced by some tags. In fact, the actual content of these informations do not matter for entity predictions and tag replacement will probably enable the algorithm to focus on more important information. Furthermore, I hesitated a lot to lowercase the text. I decided not to because it can be very informative when distinguishing a market from a Name for example. An argument against is that uppercase do not always occurr when it should and the opposite is also true. For example, some \"firm\" are displayed in complete uppercase. I should have written an appropriate function to fix this but unfortunately I did not.\n",
    "\n",
    "Other preprocessing tasks could have been used. Unfortunately, I thought about it too late in the project. The first one would have been to replace the acronyms. Articles often define acronyms to longer nominal structures and use this symbols for the rest of the article. This coreference resolution problem could be tackled by writting a function that identifies and replaces these acronyms. Another way to tackle this problem is probably more interesting: add acronym definition to the list of relations and link the relations entities via graph knowledge inferences. It shall be noted that a new acronym entity shall be added to the list as well. Finally, another issue that has not been dealt with is the one where entities are stacked next to each other without punctiation. One solution to deal with this data mining issue would be to delete or seperate symmetric strings even though that could lead to errors with existing symmetric words. One could check if the candidate word actually exists in nltk dictionary for example.\n",
    "\n",
    "The rest of the notebook formats the data the desired output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions(phrase):\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase) # 's could mean possession\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)  \n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"can't\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"n't\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"'m\", \" am\", phrase)\n",
    "    phrase = re.sub(r\"wont\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"dont\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"werent\", \"were not\", phrase)\n",
    "    phrase = re.sub(r\"'m\", \" am\", phrase)\n",
    "\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanfunc(t):\n",
    "    return clean(t,\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=False,  #if YES lowercase targets            # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=False,               # replace all numbers with a special token\n",
    "    no_digits=False,                # replace all digits with a special token\n",
    "    no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "    no_punct=False,                 # remove punctuations\n",
    "    replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUMBER>\",\n",
    "    replace_with_digit=\"0\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"                       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_uppercase(token,n):\n",
    "    token=str(token)\n",
    "    if len(token)>=n:\n",
    "        if token.isupper()==True:\n",
    "            token_=token.lower()\n",
    "            token_=token_.title()\n",
    "            l=[]\n",
    "            l.append(token)\n",
    "            l.append(token_)\n",
    "            return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessText(data):\n",
    "    for i in data:\n",
    "        for j in i['annotation']:\n",
    "            j['text']=j['text'].replace(\"?\",\"\")\n",
    "            j['text']=j['text'].replace(\"@\",\"\")\n",
    "            j['text']=j['text'].replace(\"®\",\"\")\n",
    "            j['text']=contractions(j['text'])\n",
    "            j['text']=cleanfunc(j['text'])\n",
    "            if type(rm_uppercase(j['text'],0))==list:\n",
    "                rm=rm_uppercase(j['text'],0)\n",
    "                j['text']=j['text'].replace(str(rm[0]),str(rm[1]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sents(text):\n",
    "    split=text.split('\\n')\n",
    "    sents_list=[]\n",
    "    for ss in split:\n",
    "        sents=nltk.sent_tokenize(ss)\n",
    "        for s_ in sents:\n",
    "            s_=contractions(s_)\n",
    "            s_=cleanfunc(s_)\n",
    "            n=nlp(s_)\n",
    "            d=False\n",
    "            uppercases=[]\n",
    "            for token in n:\n",
    "                if type(rm_uppercase(token,0))==list: \n",
    "                    uppercases.append(rm_uppercase(token,0))\n",
    "                if token.pos_=='VERB':\n",
    "                    d=True\n",
    "            if d==True:\n",
    "                s_=s_.replace('@','')\n",
    "                s_=s_.replace(\"®\",\"\")\n",
    "                if len(uppercases)>0:\n",
    "                    for i in range(len(uppercases)):\n",
    "                        s_=s_.replace(str(uppercases[i][0]),str(uppercases[i][1]))\n",
    "                sents_list.append(s_.replace('?','')) \n",
    "\n",
    "    return sents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comp_data(data_,label_dict):\n",
    "    data=[]\n",
    "    for i in range(len(data_)):\n",
    "        sents=tokenize_sents(data_[i]['document'])\n",
    "        for j in sents:\n",
    "            tokens=[]\n",
    "            labels=[]\n",
    "            jj=word_tokenize(j)\n",
    "            jjj=0\n",
    "            while jjj<len(jj):\n",
    "                idx_label={}\n",
    "                for l in range(len(data_[i]['annotation'])):\n",
    "                    if word_tokenize(data_[i]['annotation'][l]['text'])[0]==jj[jjj]:\n",
    "                        idx_label[str(l)]=len(word_tokenize(data_[i]['annotation'][l]['text']))\n",
    "                sorted_=sorted(idx_label.items(), key=lambda item: item[1],reverse=True)\n",
    "                global_decision=False\n",
    "                for s in sorted_:\n",
    "                    decision=False\n",
    "                    for ss in range(0,int(float(s[1]))):\n",
    "                        if s[1]>len(jj)-jjj:\n",
    "                            break\n",
    "                        elif word_tokenize(data_[i]['annotation'][int(float(s[0]))]['text'])[ss]==jj[jjj+ss]:    \n",
    "                            decision=True\n",
    "                        else:\n",
    "                            decision=False\n",
    "                    if decision==True:\n",
    "                        global_decision=True\n",
    "                        tokens.append(jj[jjj])\n",
    "                        labels.append(\"B-\"+label_dict[data_[i]['annotation'][int(float(s[0]))]['label']])\n",
    "                        for s_ in range(1,s[1]):\n",
    "                            tokens.append(jj[jjj+s_])\n",
    "                            labels.append(\"I-\"+label_dict[data_[i]['annotation'][int(float(s[0]))]['label']])\n",
    "                        jjj=jjj+s[1]\n",
    "                    else:\n",
    "                        continue\n",
    "                if global_decision==False:\n",
    "                    tokens.append(jj[jjj])\n",
    "                    labels.append('O')\n",
    "                    jjj=jjj+1\n",
    "            data.append([tokens,labels])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traintestsplit(datalabels,label_list):\n",
    "    idx = {x:i for i,x in enumerate(label_list)} \n",
    "    tr=random.sample(range(0, len(datalabels)), round(0.85*len(datalabels)))\n",
    "    tr.sort()\n",
    "    tt=[]\n",
    "    for i in range(len(datalabels)):\n",
    "        if i not in tr:\n",
    "            tt.append(i)  \n",
    "    dataset_train=[]\n",
    "    j=0\n",
    "    for i in tr:\n",
    "        d={}\n",
    "        d['index']=j\n",
    "        d['tokens']=datalabels[i][0]\n",
    "        d['label']=[idx[x] for x in datalabels[i][1] if x in idx]\n",
    "        dataset_train.append(d)\n",
    "        j=j+1\n",
    "\n",
    "    dataset_test=[]\n",
    "    j=0\n",
    "    for i in tt:\n",
    "        d={}\n",
    "        d['index']=j\n",
    "        d['tokens']=datalabels[i][0]\n",
    "        d['label']=[idx[x] for x in datalabels[i][1] if x in idx]\n",
    "        dataset_test.append(d)\n",
    "        j=j+1\n",
    "\n",
    "    dtr={}\n",
    "    dtr['data']=dataset_train\n",
    "    dtt={}\n",
    "    dtt['data']=dataset_test\n",
    "\n",
    "    return dtr, dtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('NERdata.json')\n",
    "data = json.load(f)\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "data=preprocessText(data)\n",
    "label_dict={'FIRM':\"ORG\",'PRODUCT-SERVICE':'PSM','AMOUNT':'AMNT','PERSON':'PSN', 'MARKET':'PSM','LOCATION':'LOC'}\n",
    "label_list=['O', 'B-ORG','I-ORG','B-PSM','I-PSM','B-AMNT','I-AMNT','B-PSN','I-PSN','B-LOC','I-LOC']\n",
    "datalabels=comp_data(data,label_dict)\n",
    "dtr, dtt=traintestsplit(datalabels,label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>25140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PSM</th>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PSM</th>\n",
       "      <td>873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PSN</th>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PSN</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-AMNT</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-AMNT</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "B-ORG     901\n",
       "O       25140\n",
       "B-PSM     912\n",
       "I-PSM     873\n",
       "B-LOC     139\n",
       "B-PSN     111\n",
       "I-PSN     123\n",
       "I-ORG     410\n",
       "B-AMNT     47\n",
       "I-AMNT    102\n",
       "I-LOC      61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countlabels=[]\n",
    "for i in datalabels:\n",
    "    for j in i[1]:\n",
    "        countlabels.append(j)\n",
    "\n",
    "pd.DataFrame(list(Counter(countlabels).values()), index=list(Counter(countlabels).keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"traindataNER_merged.json\", \"w\") as final:\n",
    "   json.dump(dtr, final)\n",
    "\n",
    "with open(\"testdataNER_merged.json\", \"w\") as final:\n",
    "   json.dump(dtt, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakaggle",
   "language": "python",
   "name": "condakaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
