{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romainbourgeois/miniconda3/lib/python3.8/site-packages/jax/_src/lib/__init__.py:34: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n",
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/romainbourgeois/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import json\n",
    "import nltk\n",
    "from cleantext import clean\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('labelledData/labelled_data.json')\n",
    "data = json.load(f)\n",
    "f = open('labelledData/NERdata.json')\n",
    "ner = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(object):\n",
    "    def __init__(self, data, ner, list_relations, list_rel_binary):\n",
    "        self.data=data\n",
    "        self.list_relations=list_relations\n",
    "        self.list_rel_binary=list_rel_binary\n",
    "        n={}\n",
    "        for i in ner:\n",
    "            n[i['documentName']]=[]\n",
    "            for ii in i['annotation']:\n",
    "                n[i['documentName']].append(ii['text'])\n",
    "                n[i['documentName']]=list(set(n[i['documentName']]))\n",
    "        self.ner=n # list all labeled entities\n",
    "    \n",
    "    def rel(self,d): # makes list of dictionaries indicating relationship type, child and head entity, text and whether head entity appears first in the sentence (very important for asymmetric relations)\n",
    "        rel=[]\n",
    "        ents={}\n",
    "        for e in d['tokens']:\n",
    "            ents[str(e['start'])]=e['text']\n",
    "            \n",
    "        relations=d['relations']\n",
    "        for r in relations:\n",
    "            if r['relationLabel'] in self.list_relations:\n",
    "                rel_={}\n",
    "                rel_['type']=r['relationLabel']  \n",
    "                rel_['head']=ents[str(r['head'])]\n",
    "                rel_['child']=ents[str(r['child'])]\n",
    "                rel_['documentName']=d['documentName']\n",
    "                if r['head']<=r['child']:\n",
    "                    rel_['head_first']=True\n",
    "                    rel_['sub_seq']=d['document'][r['head']:r['child']+len(rel_['child'])]\n",
    "                else:\n",
    "                    rel_['head_first']=False\n",
    "                    rel_['sub_seq']=d['document'][r['child']:r['head']+len(rel_['head'])]\n",
    "                rel.append(rel_)\n",
    "        return rel\n",
    "\n",
    "    def seq(self,d): # split between sentences\n",
    "        seqs=[]\n",
    "        split=d['document'].split('\\n')\n",
    "        for ss in split:\n",
    "            sents=nltk.sent_tokenize(ss)\n",
    "            for s in sents:\n",
    "                seqs.append(s)\n",
    "        return seqs\n",
    "\n",
    "\n",
    "    def map_fullseq(self): # basically brings rel and seq together\n",
    "        rel_=[]\n",
    "        rel=[]\n",
    "        for d in self.data:\n",
    "            rd=self.rel(d)\n",
    "            sd=self.seq(d)\n",
    "            for s in sd:\n",
    "                for r in range(len(rd)):\n",
    "                    if rd[r]['sub_seq'] in s:\n",
    "                        rd[r]['full_seq']=s\n",
    "            for rr in rd:\n",
    "                if len(rr.keys())==7:\n",
    "                    rel_.append(rr)\n",
    "        return rel_\n",
    "        \n",
    "    # preprocessing functions\n",
    "\n",
    "    def contractions(self,phrase):\n",
    "        phrase = re.sub(r\"won\\'t\", \"will not\", phrase) # 's could mean possession\n",
    "        phrase = re.sub(r\"won't\", \"will not\", phrase)  \n",
    "        phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "        phrase = re.sub(r\"can't\", \"can not\", phrase)\n",
    "        phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"n't\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"'re\", \" are\", phrase)\n",
    "        phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"'ll\", \" will\", phrase)\n",
    "        phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"'t\", \" not\", phrase)\n",
    "        phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"'ve\", \" have\", phrase)\n",
    "        phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "        phrase = re.sub(r\"'m\", \" am\", phrase)\n",
    "        phrase = re.sub(r\"wont\", \"will not\", phrase)\n",
    "        phrase = re.sub(r\"dont\", \"do not\", phrase)\n",
    "        phrase = re.sub(r\"werent\", \"were not\", phrase)\n",
    "        phrase = re.sub(r\"'m\", \" am\", phrase)\n",
    "\n",
    "        return phrase\n",
    "\n",
    "    def cleanfunc(self, t):\n",
    "        return clean(t,\n",
    "        fix_unicode=True,               # fix various unicode errors\n",
    "        to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "        lower=False,  #if YES lowercase targets            # lowercase text\n",
    "        no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "        no_urls=True,                  # replace all URLs with a special token\n",
    "        no_emails=True,                # replace all email addresses with a special token\n",
    "        no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "        no_numbers=False,               # replace all numbers with a special token\n",
    "        no_digits=False,                # replace all digits with a special token\n",
    "        no_currency_symbols=False,      # replace all currency symbols with a special token\n",
    "        no_punct=False,                 # remove punctuations\n",
    "        replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "        replace_with_url=\"<URL>\",\n",
    "        replace_with_email=\"<EMAIL>\",\n",
    "        replace_with_phone_number=\"<PHONE>\",\n",
    "        replace_with_number=\"<NUMBER>\",\n",
    "        replace_with_digit=\"0\",\n",
    "        replace_with_currency_symbol=\"<CUR>\",\n",
    "        lang=\"en\"                       \n",
    "    )\n",
    "\n",
    "    def get_ents(self,d):\n",
    "        l=[]\n",
    "        for dd in d['tokens']:\n",
    "            l.append(dd['text'])\n",
    "        return l\n",
    "\n",
    "    def get_ner_ents(self,d):\n",
    "        l=[]\n",
    "        for dd in d['annotation']:\n",
    "            l.append(dd['text'])\n",
    "        return l\n",
    "\n",
    "    def combine(self,l):  # combine all possible entities\n",
    "        ll=[]\n",
    "        for i in range(len(l)):\n",
    "            for j in range(len(l)):\n",
    "                if j==i:\n",
    "                    continue\n",
    "                else:\n",
    "                    ll.append([l[i],l[j]])\n",
    "        return ll\n",
    "\n",
    "    def remove_subEntities(self,ner): # \n",
    "        rm=[]\n",
    "        for e in ner:\n",
    "            for ee in ner:\n",
    "                if e!=ee:\n",
    "                    if e in ee:\n",
    "                        rm.append(e)\n",
    "        rm=list(set(rm))\n",
    "        for i in rm:\n",
    "            ner.remove(i)\n",
    "        return ner\n",
    "\n",
    "\n",
    "    def search_idxs(self,e,s):\n",
    "        spec=['$','¥','€','£','(',')']\n",
    "        ss=s\n",
    "        c=0\n",
    "        presence=False\n",
    "        for i in e:\n",
    "            if i in spec:\n",
    "                c=c+1 \n",
    "                presence=True     \n",
    "        if presence==True:\n",
    "            for ii in spec:\n",
    "                ss=ss.replace(ii,'')\n",
    "                e=e.replace(ii,'')\n",
    "            idx=re.search(e,ss).span()\n",
    "            idx_=[idx[0],idx[0]+c]  \n",
    "        else:\n",
    "            idx_=re.search(e,ss).span()       \n",
    "        return idx_\n",
    "        \n",
    "\n",
    "    def mapped_rel(self):\n",
    "        dataset_=[]\n",
    "        dts=self.map_fullseq() # get list of relations \n",
    "        for s in dts:\n",
    "            dataset__=[]\n",
    "            ents=[]\n",
    "            ner_=self.ner[s['documentName']]\n",
    "            ner__=[]\n",
    "            for e in ner_: \n",
    "                if e in s['full_seq']:\n",
    "                    ner__.append(e)\n",
    "            ner__=self.remove_subEntities(ner__)\n",
    "            for e in ner__:\n",
    "                if s['full_seq'].count(e)>1:\n",
    "                    ents.append(self.search_idxs(e,s['full_seq']))\n",
    "                    ss=s['full_seq'][int(self.search_idxs(e,s['full_seq'])[1]):]\n",
    "                    ents.append(self.search_idxs(e,ss))\n",
    "                    ss=ss[self.search_idxs(e,ss)[1]:]\n",
    "                    while ss.count(e)>0:\n",
    "                        ents.append(self.search_idxs(e,ss))\n",
    "                        ss=ss[self.search_idxs(e,ss)[1]:]\n",
    "                else:\n",
    "                    ents.append(self.search_idxs(e,s['full_seq']))\n",
    "            ents=self.combine(ents)\n",
    "            for i in ents:\n",
    "                h=s['full_seq'][i[0][0]:i[0][1]]  \n",
    "                t=s['full_seq'][i[1][0]:i[1][1]]\n",
    "                candidate=False\n",
    "                c={}\n",
    "                if s['type'] in self.list_rel_binary: \n",
    "                    if [h in s['head'] and t in s['child']] or [h in s['child'] and t in s['head']]:\n",
    "                        candidate=True\n",
    "                        c['sequence']=s['full_seq']\n",
    "                        c['ent1']=h\n",
    "                        c['ent2']=t\n",
    "                        c['relation']=s['type']\n",
    "                        c['head_first']=None\n",
    "\n",
    "                if s['head_first']==True: \n",
    "                    if h==s['head'] and t==s['child']:\n",
    "                        candidate=True\n",
    "                        c['sequence']=s['full_seq']\n",
    "                        c['ent1']=h\n",
    "                        c['ent2']=t\n",
    "                        c['relation']=s['type']\n",
    "                        c['head_first']=True\n",
    "\n",
    "                if s['head_first']==False: \n",
    "                    if h==s['child'] and t==s['head']:\n",
    "                        candidate=True\n",
    "                        c['sequence']=s['full_seq']\n",
    "                        c['ent1']=h\n",
    "                        c['ent2']=t\n",
    "                        c['relation']=s['type']\n",
    "                        c['head_first']=False\n",
    "\n",
    "                if candidate==False: \n",
    "                    c['sequence']=s['full_seq']\n",
    "                    c['ent1']=h\n",
    "                    c['ent2']=t\n",
    "                    c['relation']='0'\n",
    "                    c['head_first']=None\n",
    "                    candidate=True\n",
    "                \n",
    "                if candidate==True:\n",
    "                    dataset_.append(c)\n",
    "\n",
    "        return [i for n, i in enumerate(dataset_) if i not in dataset_[n + 1:]]\n",
    "\n",
    "    def build_labelled_dataset(self,proportion_test):\n",
    "        dataset_=self.mapped_rel()\n",
    "        train_data=[]\n",
    "        test_data=[]\n",
    "        datatrain={}\n",
    "        datatrain['data']=[]\n",
    "        datatest={}\n",
    "        datatest['data']=[]\n",
    "        n=len(dataset_)\n",
    "        t=random.sample(range(1, n), round(n*proportion_test))\n",
    "        for i in range(n):\n",
    "            if i in t:\n",
    "                test_data.append(dataset_[i])\n",
    "            else:\n",
    "                train_data.append(dataset_[i])\n",
    "\n",
    "        for i in range(len(train_data)):\n",
    "            trd={}\n",
    "            indxs=self.search_idxs(train_data[i]['ent1'],train_data[i]['sequence'])\n",
    "            indxs2=self.search_idxs(train_data[i]['ent2'],train_data[i]['sequence'])\n",
    "            if indxs[0]<indxs2[0]:\n",
    "                input=train_data[i]['sequence'][0:indxs[0]]+\"[E1] \"+train_data[i]['ent1']+\" [/E1] \"+train_data[i]['sequence'][indxs[1]:indxs2[0]]+\"[E2] \"+train_data[i]['ent2']+\" [/E2] \"+train_data[i]['sequence'][indxs2[1]:]\n",
    "                input=self.contractions(input)\n",
    "                input=self.cleanfunc(input)\n",
    "                input=input.split(\" \")\n",
    "                trd['index']=i\n",
    "                trd['inputs']=input\n",
    "                trd['text']=train_data[i]['sequence']\n",
    "                trd['text']=self.contractions(trd['text'])\n",
    "                trd['text']=self.cleanfunc(trd['text'])\n",
    "                trd['label']=train_data[i]['relation']\n",
    "                trd['head_first']=train_data[i]['head_first']\n",
    "            else:\n",
    "                input=train_data[i]['sequence'][0:indxs2[0]]+\"[E1] \"+train_data[i]['ent1']+\" [/E1] \"+train_data[i]['sequence'][indxs2[1]:indxs[0]]+\"[E2] \"+train_data[i]['ent2']+\" [/E2] \"+train_data[i]['sequence'][indxs[1]:]\n",
    "                input=self.contractions(input)\n",
    "                input=self.cleanfunc(input)\n",
    "                input=input.split(\" \")\n",
    "                trd['index']=i\n",
    "                trd['inputs']=input\n",
    "                trd['text']=train_data[i]['sequence']\n",
    "                trd['text']=self.contractions(trd['text'])\n",
    "                trd['text']=self.cleanfunc(trd['text'])\n",
    "                trd['label']=train_data[i]['relation']\n",
    "                trd['head_first']=train_data[i]['head_first']\n",
    "            datatrain['data'].append(trd)\n",
    "\n",
    "        for i in range(len(test_data)):\n",
    "            trd={}\n",
    "            indxs=self.search_idxs(test_data[i]['ent1'],test_data[i]['sequence'])\n",
    "            indxs2=self.search_idxs(test_data[i]['ent2'],test_data[i]['sequence'])\n",
    "            if indxs[0]<indxs2[0]:\n",
    "                input=test_data[i]['sequence'][0:indxs[0]]+\"[E1] \"+test_data[i]['ent1']+\" [/E1] \"+test_data[i]['sequence'][indxs[1]:indxs2[0]]+\"[E2] \"+test_data[i]['ent2']+\" [/E2] \"+test_data[i]['sequence'][indxs2[1]:]\n",
    "                input=self.contractions(input)\n",
    "                input=self.cleanfunc(input)\n",
    "                input=input.split(\" \")\n",
    "                trd['index']=i\n",
    "                trd['inputs']=input\n",
    "                trd['text']=train_data[i]['sequence']\n",
    "                trd['text']=self.contractions(trd['text'])\n",
    "                trd['text']=self.cleanfunc(trd['text'])\n",
    "                trd['label']=train_data[i]['relation']\n",
    "                trd['head_first']=train_data[i]['head_first']\n",
    "            else:\n",
    "                input=test_data[i]['sequence'][0:indxs2[0]]+\"[E1] \"+test_data[i]['ent1']+\" [/E1] \"+test_data[i]['sequence'][indxs2[1]:indxs[0]]+\"[E2] \"+test_data[i]['ent2']+\" [/E2] \"+test_data[i]['sequence'][indxs[1]:]\n",
    "                input=self.contractions(input)\n",
    "                input=self.cleanfunc(input)\n",
    "                input=input.split(\" \")\n",
    "                trd['index']=i\n",
    "                trd['inputs']=input\n",
    "                trd['text']=train_data[i]['sequence']\n",
    "                trd['text']=self.contractions(trd['text'])\n",
    "                trd['text']=self.cleanfunc(trd['text'])\n",
    "                trd['label']=train_data[i]['relation']\n",
    "                trd['head_first']=train_data[i]['head_first']\n",
    "\n",
    "            datatest['data'].append(trd)\n",
    "        return datatrain, datatest\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=dataset(data,ner, ['PARTNERSHIP','RESEARCH_PROJECT','SUBSIDIARY','PURCHASE','FINANCING','RECRUITMENT','LAUNCH_PRODUCT-SERVICE','HAS_PRODUCT-SERVICE',\n",
    "'OPERATES_IN_MARKET','BASED_IN','WORKS_IN'],['PARTNERSHIP'])\n",
    "mr=d.mapped_rel()\n",
    "datatrain,datatest=d.build_labelled_dataset(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reldatatrain.json\", \"w\") as final:\n",
    "   json.dump(datatrain, final)\n",
    "with open(\"reldatatest.json\", \"w\") as final:\n",
    "   json.dump(datatest, final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condakaggle",
   "language": "python",
   "name": "condakaggle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
